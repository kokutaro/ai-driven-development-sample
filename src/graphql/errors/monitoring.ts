/**
 * GraphQLã‚¨ãƒ©ãƒ¼ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ 
 *
 * ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒã§ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¨ãƒ©ãƒ¼ç›£è¦–ã€
 * ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã€ã‚¢ãƒ©ãƒ¼ãƒˆæ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
 */
import { ErrorSeverity } from './custom-errors'
import { getLogger } from './logger'

import type { BaseGraphQLError, ErrorCategory } from './custom-errors'
import type { StructuredLogger } from './logger'

/**
 * ã‚¢ãƒ©ãƒ¼ãƒˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®å‹å®šç¾©
 */
export interface AlertAction {
  config: Record<string, any>
  type: 'email' | 'slack' | 'sms' | 'webhook'
}

/**
 * ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶ã®å‹å®šç¾©
 */
export interface AlertCondition {
  actions: AlertAction[]
  condition: AlertRule
  cooldownPeriod: number // ç§’
  description: string
  enabled: boolean
  id: string
  lastTriggered?: Date
  name: string
}

/**
 * ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«ã®å‹å®šç¾©
 */
export interface AlertRule {
  groupBy?: string[]
  metric: string
  operator: 'eq' | 'gt' | 'gte' | 'lt' | 'lte'
  timeWindow: number // ç§’
  type: 'pattern' | 'rate' | 'threshold'
  value: number
}

/**
 * ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å‹å®šç¾©
 */
export interface ErrorMetrics {
  category: ErrorCategory
  count: number
  duration?: number
  errorCode: string
  fieldPath?: string
  operationName?: string
  requestId?: string
  severity: ErrorSeverity
  timestamp: string
  userId?: string
}

/**
 * ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯çµæœã®å‹å®šç¾©
 */
export interface HealthCheckResult {
  checks: Record<
    string,
    {
      duration?: number
      message?: string
      status: 'fail' | 'pass' | 'warn'
    }
  >
  metrics: {
    averageResponseTime: number
    errorRate: number
    throughput: number
  }
  service: string
  status: 'degraded' | 'healthy' | 'unhealthy'
  timestamp: string
}

/**
 * ã‚¨ãƒ©ãƒ¼ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
 */
export class GraphQLErrorMonitor {
  private readonly alertConditions = new Map<string, AlertCondition>()
  private healthCheckInterval?: NodeJS.Timeout
  private readonly healthChecks = new Map<string, Function>()
  private readonly logger: StructuredLogger
  private readonly metricsBuffer: ErrorMetrics[] = []

  private metricsFlushInterval?: NodeJS.Timeout
  private readonly metricsStore = new Map<string, ErrorMetrics[]>()

  constructor() {
    this.logger = getLogger()
    this.setupDefaultAlerts()
    this.startMetricsFlush()
    this.startHealthChecks()
  }

  /**
   * ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶ã‚’è¿½åŠ 
   */
  addAlertCondition(condition: AlertCondition): void {
    this.alertConditions.set(condition.id, condition)
  }

  /**
   * ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ
   */
  async performHealthCheck(): Promise<HealthCheckResult> {
    const result: HealthCheckResult = {
      checks: {},
      metrics: {
        averageResponseTime: 0,
        errorRate: 0,
        throughput: 0,
      },
      service: 'graphql-api',
      status: 'healthy',
      timestamp: new Date().toISOString(),
    }

    // ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
    for (const [name, checkFn] of this.healthChecks) {
      try {
        const startTime = Date.now()
        const checkResult = await checkFn()
        const duration = Date.now() - startTime

        result.checks[name] = {
          duration,
          status: checkResult ? 'pass' : 'fail',
        }
      } catch (error) {
        result.checks[name] = {
          message: error instanceof Error ? error.message : String(error),
          status: 'fail',
        }
      }
    }

    // å…¨ä½“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ±ºå®š
    const statuses = new Set(Object.values(result.checks).map((c) => c.status))
    if (statuses.has('fail')) {
      result.status = 'unhealthy'
    } else if (statuses.has('warn')) {
      result.status = 'degraded'
    }

    // ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
    result.metrics = this.calculateHealthMetrics()

    return result
  }

  /**
   * ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²
   */
  recordError(
    error: BaseGraphQLError,
    context?: {
      duration?: number
      fieldPath?: string
      operationName?: string
    }
  ): void {
    const metrics: ErrorMetrics = {
      category: error.category,
      count: 1,
      duration: context?.duration,
      errorCode: error.extensions.code as string,
      fieldPath: context?.fieldPath,
      operationName: context?.operationName,
      requestId: error.requestId,
      severity: error.severity,
      timestamp: new Date().toISOString(),
      userId: error.userId,
    }

    this.metricsBuffer.push(metrics)
    this.checkAlertConditions(metrics)

    // é«˜é‡è¦åº¦ã‚¨ãƒ©ãƒ¼ã¯å³åº§ã«ãƒ­ã‚°å‡ºåŠ›
    if (
      error.severity === ErrorSeverity.CRITICAL ||
      error.severity === ErrorSeverity.HIGH
    ) {
      this.logger.error(
        'High severity GraphQL error detected',
        error.toLogData()
      )
    }
  }

  /**
   * ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²
   */
  recordPerformance(
    operationName: string,
    duration: number,
    complexity?: number
  ): void {
    const metrics = {
      complexity,
      duration,
      operationName,
      timestamp: new Date().toISOString(),
      type: 'performance',
    }

    // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é–¾å€¤ãƒã‚§ãƒƒã‚¯
    if (duration > 5000) {
      // 5ç§’ä»¥ä¸Š
      this.logger.warn('Slow GraphQL operation detected', metrics)
    }

    this.updateTimeSeriesMetrics('performance', metrics)
  }

  /**
   * ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶ã‚’å‰Šé™¤
   */
  removeAlertCondition(id: string): void {
    this.alertConditions.delete(id)
  }

  /**
   * ç›£è¦–ã‚’åœæ­¢
   */
  stop(): void {
    if (this.metricsFlushInterval) {
      clearInterval(this.metricsFlushInterval)
    }
    if (this.healthCheckInterval) {
      clearInterval(this.healthCheckInterval)
    }
  }

  /**
   * ãƒ˜ãƒ«ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—
   */
  private calculateHealthMetrics(): HealthCheckResult['metrics'] {
    const now = Date.now()
    const oneHourAgo = now - 60 * 60 * 1000

    // éå»1æ™‚é–“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    const recentMetrics = this.metricsBuffer.filter(
      (m) => new Date(m.timestamp).getTime() > oneHourAgo
    )

    const totalRequests = recentMetrics.length || 1 // ã‚¼ãƒ­é™¤ç®—ã‚’é˜²ã
    const errorCount = recentMetrics.filter(
      (m) =>
        m.severity === ErrorSeverity.HIGH ||
        m.severity === ErrorSeverity.CRITICAL
    ).length

    const averageResponseTime =
      recentMetrics
        .filter((m) => m.duration)
        .reduce((sum, m) => sum + (m.duration || 0), 0) / totalRequests

    return {
      averageResponseTime,
      errorRate: errorCount / totalRequests,
      throughput: totalRequests / 3600, // 1æ™‚é–“ã‚ãŸã‚Šã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°
    }
  }

  /**
   * ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯
   */
  private checkAlertConditions(metrics: ErrorMetrics): void {
    for (const [id, condition] of this.alertConditions) {
      if (!condition.enabled) continue

      // ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³æœŸé–“ãƒã‚§ãƒƒã‚¯
      if (this.isInCooldown(condition)) continue

      if (this.evaluateAlertRule(condition.condition, metrics)) {
        this.triggerAlert(condition, metrics)
      }
    }
  }

  /**
   * å€¤ã®æ¯”è¼ƒ
   */
  private compareValues(
    actual: number,
    operator: string,
    expected: number
  ): boolean {
    switch (operator) {
      case 'eq': {
        return actual === expected
      }
      case 'gt': {
        return actual > expected
      }
      case 'gte': {
        return actual >= expected
      }
      case 'lt': {
        return actual < expected
      }
      case 'lte': {
        return actual <= expected
      }
      default: {
        return false
      }
    }
  }

  /**
   * ã‚¨ãƒ©ãƒ¼ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’æ¤œå‡º
   */
  private detectErrorSpike(metrics: ErrorMetrics[]): boolean {
    const recent = metrics.slice(-10) // æœ€æ–°10ä»¶
    const previous = metrics.slice(-20, -10) // ãã®å‰ã®10ä»¶

    if (previous.length === 0) return false

    const recentRate = recent.length / 10
    const previousRate = previous.length / 10

    // æœ€è¿‘ã®ã‚¨ãƒ©ãƒ¼ç‡ãŒä»¥å‰ã®2å€ä»¥ä¸Šã®å ´åˆã‚’ã‚¹ãƒ‘ã‚¤ã‚¯ã¨ã¿ãªã™
    return recentRate > previousRate * 2 && recentRate > 0.5
  }

  /**
   * ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«ã‚’è©•ä¾¡
   */
  private evaluateAlertRule(rule: AlertRule, metrics: ErrorMetrics): boolean {
    const timeWindow = rule.timeWindow * 1000 // ãƒŸãƒªç§’ã«å¤‰æ›
    const now = Date.now()
    const windowStart = now - timeWindow

    // æ™‚é–“çª“å†…ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—
    const relevantMetrics = this.metricsBuffer.filter((m) => {
      const timestamp = new Date(m.timestamp).getTime()
      return timestamp >= windowStart && timestamp <= now
    })

    switch (rule.type) {
      case 'pattern': {
        return this.evaluatePatternRule(rule, relevantMetrics)
      }
      case 'rate': {
        return this.evaluateRateRule(rule, relevantMetrics, timeWindow)
      }
      case 'threshold': {
        return this.evaluateThresholdRule(rule, relevantMetrics)
      }
      default: {
        return false
      }
    }
  }

  /**
   * ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«ã‚’è©•ä¾¡
   */
  private evaluatePatternRule(
    rule: AlertRule,
    metrics: ErrorMetrics[]
  ): boolean {
    // ç‰¹å®šã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºï¼ˆä¾‹ï¼šé€£ç¶šã—ãŸã‚¨ãƒ©ãƒ¼ã€ã‚¨ãƒ©ãƒ¼ã®æ€¥å¢—ãªã©ï¼‰
    if (rule.metric === 'consecutive_errors') {
      const consecutiveCount = this.getConsecutiveErrorCount(metrics)
      return this.compareValues(consecutiveCount, rule.operator, rule.value)
    }

    if (rule.metric === 'error_spike') {
      const isSpike = this.detectErrorSpike(metrics)
      return isSpike
    }

    return false
  }

  /**
   * ãƒ¬ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«ã‚’è©•ä¾¡
   */
  private evaluateRateRule(
    rule: AlertRule,
    metrics: ErrorMetrics[],
    timeWindow: number
  ): boolean {
    const rate = (metrics.length / timeWindow) * 1000 // 1ç§’ã‚ãŸã‚Šã®ãƒ¬ãƒ¼ãƒˆ
    return this.compareValues(rate, rule.operator, rule.value)
  }

  /**
   * é–¾å€¤ãƒ«ãƒ¼ãƒ«ã‚’è©•ä¾¡
   */
  private evaluateThresholdRule(
    rule: AlertRule,
    metrics: ErrorMetrics[]
  ): boolean {
    let value: number

    switch (rule.metric) {
      case 'critical_errors': {
        value = metrics.filter(
          (m) => m.severity === ErrorSeverity.CRITICAL
        ).length
        break
      }
      case 'error_count': {
        value = metrics.length
        break
      }
      case 'high_errors': {
        value = metrics.filter((m) => m.severity === ErrorSeverity.HIGH).length
        break
      }
      default: {
        return false
      }
    }

    return this.compareValues(value, rule.operator, rule.value)
  }

  /**
   * ã‚¢ãƒ©ãƒ¼ãƒˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
   */
  private async executeAlertAction(
    action: AlertAction,
    condition: AlertCondition,
    metrics: ErrorMetrics
  ): Promise<void> {
    switch (action.type) {
      case 'email': {
        await this.sendEmailAlert(action.config, condition, metrics)
        break
      }
      case 'slack': {
        await this.sendSlackAlert(action.config, condition, metrics)
        break
      }
      case 'sms': {
        await this.sendSmsAlert(action.config, condition, metrics)
        break
      }
      case 'webhook': {
        await this.sendWebhookAlert(action.config, condition, metrics)
        break
      }
    }
  }

  /**
   * ã‚¢ãƒ©ãƒ¼ãƒˆã®è‰²ã‚’å–å¾—
   */
  private getAlertColor(severity: ErrorSeverity): string {
    switch (severity) {
      case ErrorSeverity.CRITICAL: {
        return '#ff0000'
      }
      case ErrorSeverity.HIGH: {
        return '#ff8800'
      }
      case ErrorSeverity.LOW: {
        return '#00aa00'
      }
      case ErrorSeverity.MEDIUM: {
        return '#ffaa00'
      }
      default: {
        return '#888888'
      }
    }
  }

  /**
   * é€£ç¶šã‚¨ãƒ©ãƒ¼æ•°ã‚’å–å¾—
   */
  private getConsecutiveErrorCount(metrics: ErrorMetrics[]): number {
    let consecutiveCount = 0
    for (let i = metrics.length - 1; i >= 0; i--) {
      if (
        metrics[i].severity === ErrorSeverity.CRITICAL ||
        metrics[i].severity === ErrorSeverity.HIGH
      ) {
        consecutiveCount++
      } else {
        break
      }
    }
    return consecutiveCount
  }

  /**
   * ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³æœŸé–“ä¸­ã‹ãƒã‚§ãƒƒã‚¯
   */
  private isInCooldown(condition: AlertCondition): boolean {
    if (!condition.lastTriggered) return false

    const now = Date.now()
    const lastTriggered = condition.lastTriggered.getTime()
    const cooldownMs = condition.cooldownPeriod * 1000

    return now - lastTriggered < cooldownMs
  }

  /**
   * ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ©ãƒ¼ãƒˆã‚’é€ä¿¡ï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼‰
   */
  private async sendEmailAlert(
    config: any,
    condition: AlertCondition,
    metrics: ErrorMetrics
  ): Promise<void> {
    // å®Ÿéš›ã®å®Ÿè£…ã§ã¯å¤–éƒ¨ãƒ¡ãƒ¼ãƒ«ã‚µãƒ¼ãƒ“ã‚¹ï¼ˆSendGridã€SESãªã©ï¼‰ã‚’ä½¿ç”¨
    this.logger.info('Email alert would be sent', {
      metrics,
      subject: `GraphQL Alert: ${condition.name}`,
      to: config.recipients,
    })
  }

  /**
   * Slackã‚¢ãƒ©ãƒ¼ãƒˆã‚’é€ä¿¡
   */
  private async sendSlackAlert(
    config: any,
    condition: AlertCondition,
    metrics: ErrorMetrics
  ): Promise<void> {
    const message = {
      attachments: [
        {
          color: this.getAlertColor(metrics.severity),
          fields: [
            { short: true, title: 'Error Code', value: metrics.errorCode },
            { short: true, title: 'Category', value: metrics.category },
            { short: true, title: 'Severity', value: metrics.severity },
            {
              short: true,
              title: 'Operation',
              value: metrics.operationName || 'Unknown',
            },
            {
              short: true,
              title: 'Request ID',
              value: metrics.requestId || 'Unknown',
            },
            {
              short: true,
              title: 'User ID',
              value: metrics.userId || 'Unknown',
            },
          ],
          timestamp: Math.floor(new Date(metrics.timestamp).getTime() / 1000),
        },
      ],
      text: `ğŸš¨ GraphQL Alert: ${condition.name}`,
    }

    await fetch(config.webhookUrl, {
      body: JSON.stringify(message),
      headers: { 'Content-Type': 'application/json' },
      method: 'POST',
    })
  }

  /**
   * SMSã‚¢ãƒ©ãƒ¼ãƒˆã‚’é€ä¿¡ï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼‰
   */
  private async sendSmsAlert(
    config: any,
    condition: AlertCondition,
    metrics: ErrorMetrics
  ): Promise<void> {
    // å®Ÿéš›ã®å®Ÿè£…ã§ã¯å¤–éƒ¨SMSã‚µãƒ¼ãƒ“ã‚¹ï¼ˆTwilioã€SNSãªã©ï¼‰ã‚’ä½¿ç”¨
    this.logger.info('SMS alert would be sent', {
      message: `GraphQL Alert: ${condition.name} - ${metrics.errorCode}`,
      metrics,
      to: config.phoneNumber,
    })
  }

  /**
   * Webhookã‚¢ãƒ©ãƒ¼ãƒˆã‚’é€ä¿¡
   */
  private async sendWebhookAlert(
    config: any,
    condition: AlertCondition,
    metrics: ErrorMetrics
  ): Promise<void> {
    const payload = {
      alertId: condition.id,
      alertName: condition.name,
      metrics,
      severity: metrics.severity,
      timestamp: metrics.timestamp,
    }

    await fetch(config.url, {
      body: JSON.stringify(payload),
      headers: {
        'Content-Type': 'application/json',
        ...config.headers,
      },
      method: 'POST',
    })
  }

  /**
   * ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š
   */
  private setupDefaultAlerts(): void {
    // é«˜é‡è¦åº¦ã‚¨ãƒ©ãƒ¼ã‚¢ãƒ©ãƒ¼ãƒˆ
    this.addAlertCondition({
      actions: [
        {
          config: {
            webhookUrl: process.env.SLACK_WEBHOOK_URL || '',
          },
          type: 'slack',
        },
      ],
      condition: {
        metric: 'critical_errors',
        operator: 'gte',
        timeWindow: 300, // 5åˆ†
        type: 'threshold',
        value: 1,
      },
      cooldownPeriod: 900, // 15åˆ†
      description: 'Critical severity errors detected',
      enabled: true,
      id: 'critical-errors',
      name: 'Critical Errors',
    })

    // ã‚¨ãƒ©ãƒ¼ç‡ã‚¢ãƒ©ãƒ¼ãƒˆ
    this.addAlertCondition({
      actions: [
        {
          config: {
            url: process.env.MONITORING_WEBHOOK_URL || '',
          },
          type: 'webhook',
        },
      ],
      condition: {
        metric: 'error_count',
        operator: 'gt',
        timeWindow: 300,
        type: 'rate',
        value: 0.1, // 1ç§’ã‚ãŸã‚Š0.1ã‚¨ãƒ©ãƒ¼
      },
      cooldownPeriod: 600, // 10åˆ†
      description: 'Error rate exceeds threshold',
      enabled: true,
      id: 'error-rate',
      name: 'High Error Rate',
    })
  }

  /**
   * ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚’é–‹å§‹
   */
  private startHealthChecks(): void {
    this.healthCheckInterval = setInterval(async () => {
      const health = await this.performHealthCheck()

      if (health.status !== 'healthy') {
        this.logger.warn('Health check failed', health)
      }
    }, 30_000) // 30ç§’ã”ã¨
  }

  /**
   * ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ã‚’é–‹å§‹
   */
  private startMetricsFlush(): void {
    this.metricsFlushInterval = setInterval(() => {
      if (this.metricsBuffer.length > 0) {
        this.logger.info('Flushing metrics buffer', {
          metricsCount: this.metricsBuffer.length,
        })

        // å¤ã„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ã‚¯ãƒªã‚¢ï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’åˆ¶é™ï¼‰
        const oneHourAgo = Date.now() - 60 * 60 * 1000
        const filtered = this.metricsBuffer.filter(
          (m) => new Date(m.timestamp).getTime() > oneHourAgo
        )
        this.metricsBuffer.length = 0
        this.metricsBuffer.push(...filtered)
      }
    }, 60_000) // 1åˆ†ã”ã¨
  }

  /**
   * ã‚¢ãƒ©ãƒ¼ãƒˆã‚’ãƒˆãƒªã‚¬ãƒ¼
   */
  private async triggerAlert(
    condition: AlertCondition,
    metrics: ErrorMetrics
  ): Promise<void> {
    this.logger.warn('Alert triggered', {
      alertId: condition.id,
      alertName: condition.name,
      metrics,
    })

    condition.lastTriggered = new Date()

    // ã‚¢ãƒ©ãƒ¼ãƒˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
    for (const action of condition.actions) {
      try {
        await this.executeAlertAction(action, condition, metrics)
      } catch (error) {
        this.logger.error('Failed to execute alert action', error, {
          actionType: action.type,
          alertId: condition.id,
        })
      }
    }
  }

  /**
   * æ™‚ç³»åˆ—ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ›´æ–°
   */
  private updateTimeSeriesMetrics(key: string, data: any): void {
    if (!this.metricsStore.has(key)) {
      this.metricsStore.set(key, [])
    }

    const metrics = this.metricsStore.get(key)!
    metrics.push(data)

    // å¤ã„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å‰Šé™¤ï¼ˆ24æ™‚é–“ä»¥ä¸Šå¤ã„ï¼‰
    const oneDayAgo = Date.now() - 24 * 60 * 60 * 1000
    const filtered = metrics.filter(
      (m) => new Date(m.timestamp).getTime() > oneDayAgo
    )
    this.metricsStore.set(key, filtered)
  }
}

/**
 * ã‚°ãƒ­ãƒ¼ãƒãƒ«ç›£è¦–ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
 */
let globalMonitor: GraphQLErrorMonitor | null = null

/**
 * ã‚°ãƒ­ãƒ¼ãƒãƒ«ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚’å–å¾—
 */
export function getErrorMonitor(): GraphQLErrorMonitor {
  if (!globalMonitor) {
    globalMonitor = initializeErrorMonitoring()
  }
  return globalMonitor
}

/**
 * ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–
 */
export function initializeErrorMonitoring(): GraphQLErrorMonitor {
  if (!globalMonitor) {
    globalMonitor = new GraphQLErrorMonitor()
  }
  return globalMonitor
}
